{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# First Break Picking Prediction Demo\n",
    "\n",
    "This notebook shows how to generate predictions for a new dataset using a pretrained model. Before\n",
    "following it, we strongly suggest following the training notebook (`fbp_train_with_api.ipynb`)\n",
    "located in the same folder. The cells below will use the model trained by that notebook by default.\n",
    "\n",
    "Note that we do not retrain the provided model at all here, and assume that the new dataset is\n",
    "\"inside the distribution\" of the training data that was previously used. If this is not the case,\n",
    "the predictions may be completely useless.\n",
    "\n",
    "Finally, note that this example will once again not use an external configuration file. Instead, it\n",
    "will directly invoke the functions and class constructors that would use the content of such a\n",
    "configuration file. This will help clarify the link between the content of these files, the role\n",
    "of each parameter, and the step where they are involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports are centralized here (helps identify environment issues before doing anything else!)\n",
    "\n",
    "# these packages are part of the 'standard' library, and should be available in all environments\n",
    "import functools\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# these packages are 3rd-party dependencies, and must be installed via pip or anaconda\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.utils.data\n",
    "import tqdm\n",
    "\n",
    "# these packages are part of our API and must be manually installed (see the top-level README.md)\n",
    "import hardpicks\n",
    "import hardpicks.data.fbp.data_module as fbp_data_module\n",
    "import hardpicks.data.fbp.gather_transforms as fbp_data_transforms\n",
    "import hardpicks.metrics.fbp.utils as metrics_utils\n",
    "import hardpicks.models.fbp.utils as model_utils\n",
    "import hardpicks.models.fbp.unet as fbp_unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model reloading\n",
    "\n",
    "Instantiating a pretrained model is usually done in two steps: first, we need to use the\n",
    "configuration of the model to recreate an identical copy of the network architecture that\n",
    "was trained. Second, we need to ask PyTorch to reload the model's weights (i.e. the parameters\n",
    "that were fitted during training) into that new copy of the model.\n",
    "\n",
    "Here, since we used PyTorch-Lightning for training, this is simplified to one step. Under the hood,\n",
    "PyTorch-Lightning manages a copy of the model's configuration directly inside the checkpoint. Thus,\n",
    "our job is really simplified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_path = \"output/notebook_train_example/\"  # this path is the one from the 1st demo!\n",
    "\n",
    "print(f\"Parsing pretrained model artifacts from: {pretrained_model_path}\")\n",
    "assert os.path.isdir(pretrained_model_path), \\\n",
    "    f\"invalid pretrained model directory path: {pretrained_model_path}\"\n",
    "# the pretrained model weights file (or \"checkpoint\") has a name that varies a bit, we'll glob it\n",
    "model_ckpt_path_pattern = os.path.join(pretrained_model_path, \"best*.ckpt\")\n",
    "model_ckpt_paths = glob.glob(model_ckpt_path_pattern)\n",
    "assert len(model_ckpt_paths) >= 1, \\\n",
    "    f\"could not locate at least one 'best' checkpoint using: {model_ckpt_path_pattern}\"\n",
    "model_ckpt_path = model_ckpt_paths[-1]  # arbitrary: we'll keep the last if there are many\n",
    "assert os.path.isfile(model_ckpt_path), \\\n",
    "    f\"invalid checkpoint path: {model_ckpt_path}\"\n",
    "\n",
    "# alright, time to reinstantiate the model!\n",
    "model = fbp_unet.FBPUNet.load_from_checkpoint(model_ckpt_path)\n",
    "\n",
    "param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Reinstantiated a pretrained U-Net model with {(param_count / 1000000):2.1f}M parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "In this case, we will load the Denare Beach data from scratch, and generate predictions for it.\n",
    "Luckily, since the Denare Beach HDF5 archives are structured the same way as the HDF5 archives\n",
    "we previously used, we can once again rely on our gather parser implementation. The only new thing\n",
    "we need to define is a \"site info\" dictionary that will provide basic info such as where the data\n",
    "is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root_path = hardpicks.FBP_DATA_DIR  # change this path if your data is located elsewhere!\n",
    "denare_dir_path = os.path.join(dataset_root_path, \"Denare_2D\")\n",
    "denare_hdf5_path = os.path.join(denare_dir_path, \"Denare_beach_dynamite_geom_2s_for_Mila.hdf\")\n",
    "assert os.path.isfile(denare_hdf5_path), \\\n",
    "    f\"could not locate the Denare Beach HDF5 file at: {denare_hdf5_path}\"\n",
    "print(f\"Will parse HDF5 data from: {denare_hdf5_path}\")\n",
    "\n",
    "# as long as the HDF5 structure is the same way as the previous, we can use the existing parser!\n",
    "# ... we just need to define the proper site info dictionary and config and pass them to the API\n",
    "test_site_info = {\n",
    "    \"site_name\": \"Denare\",  # the name used to identify this site in logs/tables/plots/etc.\n",
    "    \"raw_hdf5_path\": denare_hdf5_path,  # the path where the raw hdf5 file can be found\n",
    "    \"processed_hdf5_path\": denare_hdf5_path,  # we won't be preprocessing the raw data, so same as above\n",
    "    \"receiver_id_digit_count\": 3,  # this is used to decompose pegs into unique receiver identifiers\n",
    "    \"first_break_field_name\": \"SPARE1\",  # specified in case the dataset provides multiple picks per trace\n",
    "    \"raw_md5_checksum\": \"332874c28971ab8029ac52bb9480fe0f\",  # to make sure we're not using corrupted data\n",
    "    \"processed_md5_checksum\": \"332874c28971ab8029ac52bb9480fe0f\",  # same as above!\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The rest of the data loading from now on is similar to what was done in the training demo notebook.\n",
    "\n",
    "**NOTE**: it is important to remember that if the pretrained model used a particular set of\n",
    "preprocessing operations to prepare its input data, we should be using the SAME operations here.\n",
    "Diverting from these would take the input data \"out-of-distribution\", and the predictions would\n",
    "suffer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_site_params = dict(\n",
    "    convert_to_fp16=True,  # convert trace sample data to 16-bit floats (saves memory!)\n",
    "    convert_to_int16=True,  # same as above, but for identifiers, picks, and other integer data\n",
    "    preload_trace_data=True,  # we'll put everything in memory right away (should be <5GB)\n",
    "    cache_trace_metadata=True,\n",
    "    provide_offset_dists=True,  # finally, we'll generate new offset distance arrays/maps\n",
    ")\n",
    "\n",
    "test_data_parser = fbp_data_module.FBPDataModule.create_parser(\n",
    "    site_info=test_site_info,\n",
    "    site_params={\n",
    "        \"use_cache\": False,\n",
    "        \"normalize_samples\": True,  # this was used during training, we need to reuse it here too!\n",
    "    },\n",
    "    prefix=\"test\",\n",
    "    dataset_hyper_params=generic_site_params,\n",
    "    segm_class_count=model.segm_class_count,\n",
    ")\n",
    "print(f\"Test dataset parser ready with {len(test_data_parser)} gathers!\")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_data_parser,\n",
    "    batch_size=6,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=functools.partial(\n",
    "        fbp_data_module.fbp_batch_collate,\n",
    "        pad_to_nearest_pow2=True,\n",
    "    ),\n",
    ")\n",
    "print(f\"Test data loader ready with {len(test_data_loader)} minibatches!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once again, let's display an actual minibatch of random line gathers as a grid of 2D images\n",
    "fig, axes = plt.subplots(6, figsize=(12, 12))\n",
    "minibatch = next(iter(test_data_loader))\n",
    "for gather_idx in range(6):\n",
    "    # we'll use some utility functions that are already-written to convert gathers into images\n",
    "    gather_image = model_utils.generate_pred_image(\n",
    "        # note: the provided first breaks picks will be shown in green\n",
    "        batch=minibatch,\n",
    "        raw_preds=None,\n",
    "        batch_gather_idx=gather_idx,\n",
    "        segm_class_count=model.segm_class_count,\n",
    "        segm_first_break_prob_threshold=0.,\n",
    "        draw_prior=False,\n",
    "    )\n",
    "    ax = axes[gather_idx]\n",
    "    ax.imshow(\n",
    "        gather_image,\n",
    "        interpolation=\"none\",\n",
    "        aspect=\"auto\",\n",
    "    )\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prediction\n",
    "\n",
    "To generate predictions, we could delegate everything to PyTorch-Lightning (and deal with how\n",
    "it expects to receive/return the data), or call the underlying prediction functions directly\n",
    "(PyTorch-style). Here, we will do the latter, but using the input tensor preparation function\n",
    "that's already implemented with the model, and that PyTorch-Lightning would also rely on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count():\n",
    "    print(\"Will predict on GPU.\")\n",
    "    model = model.cuda()\n",
    "else:\n",
    "    print(\"Will predict on CPU.\")\n",
    "\n",
    "model.test_evaluator.reset()  # each time this cell is executed, we'll reset the evaluator...\n",
    "\n",
    "loader_wrapper = tqdm.tqdm(test_data_loader, total=len(test_data_loader))\n",
    "for batch_idx, batch in enumerate(loader_wrapper):  # loops over all minibatches in the test data loader\n",
    "    with torch.no_grad():  # since we don't want to do backpropagation and track gradients like in training\n",
    "        input_tensor = model_utils.prepare_input_features(\n",
    "            batch,\n",
    "            use_dist_offsets=model.use_dist_offsets,\n",
    "            use_first_break_prior=model.use_first_break_prior,\n",
    "        ).to(model.device).float()\n",
    "        predictions = model(input_tensor)  # calls the forward pass of the model\n",
    "        # NOTE: since our pre-trained model is a segmentation model (i.e. a U-Net encoder-decoder),\n",
    "        # the \"predictions\" are actually a stack of class score maps, one for each of the gathers in\n",
    "        # the provided minibatch. The shape of the 'predictions' tensor is thus:\n",
    "        #    predictions.shape = (BATCH_SIZE, CLASS_COUNT, TRACE_COUNT, SAMPLE_COUNT)\n",
    "        # ... to get actual first break pick predictions from this map, we need to search for the\n",
    "        # sample in each trace (row) that maximizes the score of the \"first break\" class. We provide\n",
    "        # a function to do this:\n",
    "        predicted_picks = metrics_utils.get_regr_preds_from_raw_preds(\n",
    "            raw_preds=predictions,\n",
    "            segm_class_count=model.segm_class_count,\n",
    "            prob_threshold=0.01,  # this sets the \"minimum bar\" for the confidence in a first break!\n",
    "        )\n",
    "        # ... our metrics evaluator actually calls that function under the hood!\n",
    "        metrics = model.test_evaluator.ingest(batch, batch_idx, predictions.detach())\n",
    "        # ... if we wanted to do something else with the predictions, we would do it here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A useful part of the already-implemented model objects is that they contain a test set evaluator\n",
    "with the metrics we were already using during training. This evaluator has been provided with the\n",
    "model predictions at every step of the loop above, meaning we can just print how well the model did!\n",
    "\n",
    "**NOTE**: this is based on the assumption that the groundtruth data was also packaged and passed as\n",
    "a component of the minibatches by the data loader, and that it can be used by the \"ingest\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_map = model.test_evaluator.summarize()\n",
    "print(f\"Test results for {test_site_info['site_name']}:\")\n",
    "for key, val in test_results_map.items():\n",
    "    print(f\"\\t{key}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finally, let's display predictions on top of the images we were showing earlier..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(6, figsize=(12, 24))\n",
    "minibatch = next(iter(test_data_loader))\n",
    "for gather_idx in range(6):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = model_utils.prepare_input_features(\n",
    "            minibatch,\n",
    "            use_dist_offsets=model.use_dist_offsets,\n",
    "            use_first_break_prior=model.use_first_break_prior,\n",
    "        ).to(model.device).float()\n",
    "        predictions = model(input_tensor)  # calls the forward pass of the model\n",
    "    gather_image = model_utils.generate_pred_image(\n",
    "        # note: the provided first breaks picks will be shown in green, predictions in red\n",
    "        batch=minibatch,\n",
    "        raw_preds=predictions,\n",
    "        batch_gather_idx=gather_idx,\n",
    "        segm_class_count=model.segm_class_count,\n",
    "        segm_first_break_prob_threshold=0.,\n",
    "        draw_prior=False,\n",
    "        draw_prob_heatmap=False,\n",
    "    )\n",
    "    ax = axes[gather_idx]\n",
    "    ax.imshow(\n",
    "        gather_image,\n",
    "        interpolation=\"none\",\n",
    "        aspect=\"auto\",\n",
    "    )\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Bonus**: as mentioned in our report, there are tons of ways to improve the quality of first break\n",
    "picking predictions. Most of those are related to the training of the model, so we cannot do that\n",
    "here. However, creating an ensemble is a test-time improvement that we can do. However, since\n",
    "we only have a single model, we cannot create a conventional \"model ensemble\". Instead, we will\n",
    "create an ensemble based on the augmentation of input gathers.\n",
    "\n",
    "To keep things simple, we will only apply a single augmentation operations to our input gathers,\n",
    "namely a trace-wise flip. This will create a pair of inputs whose predictions will be averaged,\n",
    "resulting in (hopefully) a small boost in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create a 2nd data loader with a slightly modified collate function...\n",
    "def flip_then_collate(list_of_gathers):\n",
    "    for gather in list_of_gathers:\n",
    "        # we flip gathers using an already implemented function that also flips metadata as needed!\n",
    "        fbp_data_transforms.flip(gather)\n",
    "    return fbp_data_module.fbp_batch_collate(\n",
    "        list_of_gathers,\n",
    "        pad_to_nearest_pow2=True,\n",
    "    )\n",
    "\n",
    "tta_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_data_parser,\n",
    "    batch_size=6,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=flip_then_collate,\n",
    ")\n",
    "\n",
    "# important note: we add padding AFTER the flip, so we'll need to be careful when unflipping below!\n",
    "\n",
    "model.test_evaluator.reset()  # restart with a fresh evaluator\n",
    "loader_wrapper = tqdm.tqdm(\n",
    "    zip(test_data_loader, tta_data_loader),\n",
    "    total=len(test_data_loader),\n",
    ")\n",
    "for batch_idx, (batch, batch_flip) in enumerate(loader_wrapper):\n",
    "    with torch.no_grad():\n",
    "        # the disadvantage to test-time-augmentation: we need to call the model multiple times...\n",
    "        input_tensor = model_utils.prepare_input_features(\n",
    "            batch,\n",
    "            use_dist_offsets=model.use_dist_offsets,\n",
    "            use_first_break_prior=model.use_first_break_prior,\n",
    "        ).to(model.device).float()\n",
    "        predictions = model(input_tensor).detach()\n",
    "\n",
    "        # once again for the flipped batches...\n",
    "        input_tensor_flip = model_utils.prepare_input_features(\n",
    "            batch_flip,\n",
    "            use_dist_offsets=model.use_dist_offsets,\n",
    "            use_first_break_prior=model.use_first_break_prior,\n",
    "        ).to(model.device).float()\n",
    "        predictions_flip = model(input_tensor_flip).detach()\n",
    "\n",
    "        assert predictions_flip.shape == predictions.shape\n",
    "\n",
    "        # now we only need to combine the two prediction maps into a single one!\n",
    "        # (combining classification scores can be easily done by averaging!)\n",
    "\n",
    "        predictions_unflip = torch.flip(predictions_flip, [2])  # dim#2 = trace axis\n",
    "        for gather_idx in range(len(predictions_flip)):\n",
    "            # note: we need to unflip the right traces without touching the padding...\n",
    "            expected_trace_count = batch_flip[\"trace_count\"][gather_idx].item()\n",
    "            expected_padding_count = predictions_flip.shape[2] - expected_trace_count\n",
    "            predictions[gather_idx, :, 0:expected_trace_count, :] = torch.mean(\n",
    "                torch.stack([\n",
    "                    predictions[gather_idx, :, 0:expected_trace_count, :],\n",
    "                    predictions_unflip[gather_idx, :, expected_padding_count:, :]\n",
    "                ]),\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "        metrics = model.test_evaluator.ingest(batch, batch_idx, predictions)\n",
    "\n",
    "# finally, let's see if the results improved!\n",
    "\n",
    "tta_results_map = model.test_evaluator.summarize()\n",
    "print(f\"Test results (with TTA) for {test_site_info['site_name']}:\")\n",
    "for key, val in tta_results_map.items():\n",
    "    diff = val - test_results_map[key]\n",
    "    print(f\"\\t{key}: {val}  ({diff:+1.4f} difference with original)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
