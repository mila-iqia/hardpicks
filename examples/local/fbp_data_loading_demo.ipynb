{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Loading Demo\n",
    "\n",
    "This notebook shows some of the steps required to load the \"raw\" first break picking data (gathers\n",
    "and annotations without using other modules from this package) to be used to train predictive models.\n",
    "\n",
    "We assume that the raw data (`.hdf5.xz` files) is available in a `raw` directory located\n",
    "in the subfolder structure of the overall project's `data` directory. Feel free to modify this\n",
    "base path in the cell below if you do not use the same folder structure.\n",
    "\n",
    "For the links to download the raw data, refer to the repository's top-level README file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run me once to import all the packages needed for the notebook to run\n",
    "import collections  # used for namedtuples in site info definitions\n",
    "import os  # required for path/filesystem stuff\n",
    "import hashlib  # required for MD5 checksum analysis\n",
    "import lzma  # required for dataset decompression\n",
    "import typing  # for typedefs in class/function/args declarations\n",
    "\n",
    "import h5py  # required for HDF5 file parsing\n",
    "import matplotlib.pyplot as plt  # used in the last cell to render a gather image\n",
    "import numpy as np  # all loaded arrays will be provided as numpy arrays\n",
    "import torch.utils.data  # required for the trace dataset parser class definition\n",
    "\n",
    "data_root_path = \"../../data/fbp/data/raw\"\n",
    "if not os.path.exists(data_root_path):\n",
    "    raise AssertionError(\"could not locate raw data root directory; edit me!\")\n",
    "\n",
    "print(\"HardPicks Demo Notebook v1.0 ready to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, we provide a few high-level parameter structures that are associated with each dataset; these\n",
    "are useful when parsing the data and making sure it is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SiteInfo = collections.namedtuple(  # each site will be defined using this tuple definition...\n",
    "    \"SiteInfo\",\n",
    "    [\n",
    "        \"site_name\",  # the 'name' of the site (for lookup/display/printing)\n",
    "        \"first_break_field_name\",  # the name of the HDF5 dataset/field containing the first breaks \n",
    "        \"hdf5_file_name\",  # the original name expected for the HDF5 file itself \n",
    "        \"md5_checksum\",  # the checksum of the original HDF5 file (as of the initial release date)\n",
    "        \"receiver_id_digit_count\",  # the number of digits used to identify receivers in the dataset\n",
    "    ],\n",
    ")\n",
    "\n",
    "BRUNSWICK_SITE_INFO = SiteInfo(\n",
    "    site_name=\"Brunswick\",\n",
    "    first_break_field_name=\"SPARE1\",\n",
    "    hdf5_file_name=\"Brunswick_orig_1500ms_V2.hdf5\",\n",
    "    md5_checksum=\"3ca7b8d1633ec7cecc07f0eff94dff69\",\n",
    "    receiver_id_digit_count=3,\n",
    ")\n",
    "\n",
    "HALFMILE_SITE_INFO = SiteInfo(\n",
    "    site_name=\"Halfmile\",\n",
    "    first_break_field_name=\"SPARE1\",\n",
    "    hdf5_file_name=\"Halfmile3D_add_geom_sorted.hdf5\",\n",
    "    md5_checksum=\"dc7b0d181b8b81109a7e2e0ad60fa391\",\n",
    "    receiver_id_digit_count=4,\n",
    ")\n",
    "\n",
    "LALOR_SITE_INFO = SiteInfo(\n",
    "    site_name=\"Lalor\",\n",
    "    first_break_field_name=\"SPARE2\",\n",
    "    hdf5_file_name=\"Lalor_raw_z_1500ms_norp_geom_v3.hdf5\",\n",
    "    md5_checksum=\"d3c4722ef791cc3ec5adab656b16b925\",\n",
    "    receiver_id_digit_count=3,\n",
    ")\n",
    "\n",
    "SUDBURY_SITE_INFO = SiteInfo(\n",
    "    site_name=\"Sudbury\",\n",
    "    first_break_field_name=\"SPARE1\",\n",
    "    hdf5_file_name=\"preprocessed_Sudbury3D.hdf\",\n",
    "    md5_checksum=\"f8772ec3a650820db3da1c8936f6945c\",\n",
    "    receiver_id_digit_count=3,\n",
    ")\n",
    "\n",
    "SITE_INFO_ARRAY = [\n",
    "    SUDBURY_SITE_INFO,\n",
    "    HALFMILE_SITE_INFO,\n",
    "    BRUNSWICK_SITE_INFO,\n",
    "    LALOR_SITE_INFO,\n",
    "]\n",
    "\n",
    "print(\"Site info structures initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will now scan the data directory for all the dataset files and uncompress them if any are found\n",
    "and not yet uncompressed. Note that the total sized of the uncompressed files that will be created\n",
    "is around 45 GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_md5_hash(file_path):\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "dataset_file_names = [p for p in os.listdir(data_root_path) if \".hdf\" in p]\n",
    "dataset_file_paths = [os.path.join(data_root_path, p) for p in dataset_file_names]\n",
    "assert len(dataset_file_paths) > 0, f\"could not locate any hdf5 file in: {data_root_path}\"\n",
    "\n",
    "warned_already = False\n",
    "for dataset_file_path in dataset_file_paths:\n",
    "    assert any([dataset_file_path.endswith(s) for s in [\".xz\", \".hdf\", \".hdf5\"]])\n",
    "    if dataset_file_path.endswith(\".xz\"):\n",
    "        uncompr_dataset_path = dataset_file_path.split(\".xz\")[0]\n",
    "        if not os.path.exists(uncompr_dataset_path):\n",
    "            print(f\"Decompressing dataset: {uncompr_dataset_path}\")\n",
    "            if not warned_already:\n",
    "                print(\"\\tnote: this can take several minutes per dataset!\")\n",
    "                warned_already = True\n",
    "            with lzma.open(dataset_file_path) as lzfd:\n",
    "                with open(uncompr_dataset_path, \"wb\") as outfd:\n",
    "                    for chunk in iter(lambda: lzfd.read(4096), b\"\"):\n",
    "                        outfd.write(chunk)\n",
    "\n",
    "for site_info in SITE_INFO_ARRAY:\n",
    "    print(f\"Validating MD5 for {site_info.site_name}...\")\n",
    "    data_file_path = os.path.join(data_root_path, site_info.hdf5_file_name)\n",
    "    assert os.path.isfile(data_file_path), f\"could not locate HDF5 file: {data_file_path}\"\n",
    "    curr_hash = get_md5_hash(data_file_path)\n",
    "    assert curr_hash == site_info.md5_checksum, \"checksum validation failed\"\n",
    "\n",
    "print(\"All datasets validated and ready-to-go.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The traces and first break annotations are contained in the HDF5 files that are now ready to be\n",
    "parsed. The HDF5 file holds this data under the \"TRACE_DATA/DEFAULT\" group. Below is the complete\n",
    "list of data that are expected to be present under different field names. The function also provided\n",
    "below will make sure all these fields are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_EXPECTED_HDF5_FIELDS = [\n",
    "    \"REC_PEG\",  # PEG identifiers for the receivers (one entry per trace)\n",
    "    \"REC_X\",  # X coordinate of the receivers (one entry per trace)\n",
    "    \"REC_Y\",  # Y coordinate of the receivers (one entry per trace)\n",
    "    \"REC_HT\",  # height/elevation of the receivers (one entry per trace)\n",
    "    \"SAMP_NUM\",  # sample counts (one entry per trace, but should be constant)\n",
    "    \"SAMP_RATE\",  # sampling rates (one entry per trace, but should be constant)\n",
    "    \"COORD_SCALE\",  # scale factors to be applied to coordinates (one entry per trace, but should be constant)\n",
    "    \"HT_SCALE\",  # scale factors to be applied to heights/evalations (one entry per trace, but should be constant)\n",
    "    \"SHOTID\",  # shot identifiers (one entry per trace)\n",
    "    \"SHOT_PEG\",  # shot PEG identifiers (one entry per trace)\n",
    "    \"SOURCE_X\",  # X coordinate of the shot location (one entry per trace)\n",
    "    \"SOURCE_Y\",  # Y coordinate of the shot location (one entry per trace)\n",
    "    \"SOURCE_HT\",  # height/elevation of the shot location (one entry per trace)\n",
    "    \"data_array\",  # 2D array of all recorded seismic traces (trace count x sample count)\n",
    "]\n",
    "\n",
    "\n",
    "def check_h5data_struct(h5root: h5py.Group, expected_hdf5_fields: typing.List[str]) -> None:\n",
    "    \"\"\"Parses the top-level hdf5 objects and validates the dataset structure.\"\"\"\n",
    "    assert \"TRACE_DATA\" in h5root, \"unexpected root level hdf5 structure\"\n",
    "    assert \"DEFAULT\" in h5root[\"TRACE_DATA\"], \"unexpected root level hdf5 structure\"\n",
    "    h5data = h5root[\"TRACE_DATA\"][\"DEFAULT\"]\n",
    "    expected_trace_count = None\n",
    "    for expected_field in expected_hdf5_fields:\n",
    "        assert (\n",
    "            expected_field in h5data\n",
    "        ), f\"missing field {expected_field} in HDF5 file\"\n",
    "        if expected_trace_count is None:\n",
    "            expected_trace_count = len(h5data[expected_field])\n",
    "        else:\n",
    "            assert expected_trace_count == len(h5data[expected_field]), (\n",
    "                f\"unexpected dataset size for field {expected_field}\\n\"\n",
    "                f\"\\t({len(h5data[expected_field])} instead of {expected_trace_count})\"\n",
    "            )\n",
    "\n",
    "for site_info in SITE_INFO_ARRAY:\n",
    "    print(f\"Checking HDF5 structure for {site_info.site_name}...\")\n",
    "    data_file_path = os.path.join(data_root_path, site_info.hdf5_file_name)\n",
    "    expected_hdf5_fields = BASE_EXPECTED_HDF5_FIELDS + [site_info.first_break_field_name]\n",
    "    with h5py.File(data_file_path, mode=\"r\") as h5fd:\n",
    "        check_h5data_struct(h5fd, expected_hdf5_fields)\n",
    "\n",
    "print(\"All datasets checked for HDF5 structure and fields.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Since all the datasets have a common structure, we should define a 'parser' class to encapsulate\n",
    "the parsing logic; we do that below using the PyTorch dataset interface. See the docstring in\n",
    "that class for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraceDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Container for raw FBP trace data read directly from an HDF5 file.\n",
    "\n",
    "    The `size` of this parser will be equal to the total number of traces in the dataset. Each trace\n",
    "    in the dataset will be assigned a unique identifier (an integer between 0 and SIZE-1) making it\n",
    "    possible to directly iterate over all traces. The `__getitem__` function will return a\n",
    "    dictionary of all the data associated with a particular trace.\n",
    "    \n",
    "    The class attributes detailed below with the `_map` suffix can be used to assemble gathers. This\n",
    "    parser may not be very efficient for the generation of images however, so if you intend on\n",
    "    training models for a while, you might want to consider wrapping it and caching its results. \n",
    "\n",
    "    Attributes:\n",
    "        receiver_id_digit_count: defines how many digits will be imputed to the receiver ids in\n",
    "            the encoded receiver peg number.\n",
    "        first_break_field_name: The variable name in the HDF5 file where the picks are stored.\n",
    "        total_trace_count: the total number of traces (equal to the number of deployed receivers).\n",
    "        trace_to_shot_map: maps all trace IDs to their corresponding unique shot ID.\n",
    "        shot_to_trace_map: maps all shot IDs to their corresponding list of trace IDs.\n",
    "        trace_to_line_map: maps all trace IDs to their corresponding unique receiver line ID.\n",
    "        line_to_trace_map: maps all receiver line IDs to their corresponding list of trace IDs.\n",
    "        trace_to_rec_map: maps all trace IDs to their corresponding unique receiver ID.\n",
    "        rec_to_trace_map: maps all receiver IDs to their corresponding list of trace IDs.\n",
    "        trace_to_gather_map: maps all trace IDs to their corresponding unique line gather ID.\n",
    "        gather_to_trace_map: maps all line gather IDs to their corresponding list of trace IDs.\n",
    "        samp_num: the number of seismic samples recorded per trace.\n",
    "        samp_rate: the sampling rate of receivers (in microseconds).\n",
    "        first_break_labels: the sample indices that were picked as the 'first break' by an\n",
    "            annotator or automated tool. There is one value per trace.\n",
    "        first_break_timestamps: the timestamps (in milliseconds) that were picked as the 'first\n",
    "            break' by an annotator or automated tool. There is one value per trace.\n",
    "        coord_scale: scaling factor to be applied to the parsed receiver/shot X and Y coordinates to\n",
    "            get their real coordinates.\n",
    "        ht_scale: scaling factor to be applied to the parsed receiver/shot Z (height) coordinate to\n",
    "            get their real coordinates.\n",
    "        shot_to_coords_map: maps all shot IDs to their corresponding 3d coordinates (X,Y,Z).\n",
    "        rec_to_coords_map: maps all receiver IDs to their corresponding 3d coordinates (X,Y,Z).\n",
    "    \"\"\"\n",
    "\n",
    "    BAD_FIRST_BREAK_PICK_INDEX = 0  # this constant is used as a way to flag 'bad' annotations\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hdf5_path: typing.AnyStr,\n",
    "        receiver_id_digit_count: int,  # the digit count allocated for IDs in the rec peg number\n",
    "        first_break_field_name: typing.AnyStr,  # the variable name in the HDF5 file where the picks are stored\n",
    "    ):\n",
    "        \"\"\"Building the object will actually open and parse the hdf5 archive.\"\"\"\n",
    "        print(f\"Parsing HDF5 file at: {hdf5_path}\")\n",
    "        with h5py.File(hdf5_path, mode=\"r\") as h5root:\n",
    "            h5data = h5root[\"TRACE_DATA\"][\"DEFAULT\"]\n",
    "            self.receiver_id_digit_count = receiver_id_digit_count\n",
    "            self.first_break_field_name = first_break_field_name\n",
    "            self.hdf5_path = hdf5_path\n",
    "            self.samp_num = self._get_const_parameter(h5data, \"SAMP_NUM\")\n",
    "            self.samp_rate = self._get_const_parameter(h5data, \"SAMP_RATE\")  # in usec (typically?)\n",
    "            self.max_fb_timestamp = self.samp_num * self.samp_rate / 1000  # in msec (based on above?)\n",
    "            self.coord_scale = self._get_const_parameter(h5data, \"COORD_SCALE\")\n",
    "            self.ht_scale = self._get_const_parameter(h5data, \"HT_SCALE\")\n",
    "            self.total_trace_count = len(h5data[\"data_array\"])\n",
    "            print(f\"\\tfound {self.total_trace_count} traces\")\n",
    "            self.trace_to_shot_map, self.shot_to_trace_map = self._get_id_maps(\n",
    "                h5data, \"SHOT_PEG\", decode_peg_id=True, id_digit_count=0\n",
    "            )  # no need for digit count\n",
    "            print(f\"\\tfound {len(self.shot_to_trace_map)} shots\")\n",
    "            self.trace_to_rec_map, self.rec_to_trace_map = self._get_id_maps(\n",
    "                h5data,\n",
    "                \"REC_PEG\",\n",
    "                decode_peg_id=False,\n",
    "                id_digit_count=receiver_id_digit_count,\n",
    "            )\n",
    "            print(f\"\\tfound {len(self.rec_to_trace_map)} receivers\")\n",
    "            self.trace_to_line_map, self.line_to_trace_map = self._get_id_maps(\n",
    "                h5data,\n",
    "                \"REC_PEG\",\n",
    "                decode_peg_id=True,\n",
    "                id_digit_count=receiver_id_digit_count,\n",
    "            )\n",
    "            print(f\"\\tfound {len(self.line_to_trace_map)} receiver lines\")\n",
    "            self.trace_to_gather_map, self.gather_to_trace_map = self._get_gather_maps()\n",
    "            # note: number of 'line gathers' = number of receiver lines x number of shots\n",
    "            print(f\"\\tfound {len(self.gather_to_trace_map)} line gathers\")\n",
    "\n",
    "            self.first_break_labels, self.first_break_timestamps = self._get_first_breaks(\n",
    "                h5data, self.first_break_field_name\n",
    "            )\n",
    "            self.rec_to_coords_map = self._get_coords(h5data, \"REC\")\n",
    "            self.shot_to_coords_map = self._get_coords(h5data, \"SHOT\")\n",
    "        self._h5fd = None  # will be set in the first getitem function call for later queries\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_id_maps(\n",
    "        h5data: h5py.Group,\n",
    "        peg_name: typing.AnyStr,\n",
    "        decode_peg_id: bool,\n",
    "        id_digit_count: int,\n",
    "    ) -> typing.Tuple[typing.Sequence[int], typing.Dict[int, typing.Sequence[int]]]:\n",
    "        \"\"\"Returns both the trace-to-id and id-to-trace mappings for receivers or shots.\"\"\"\n",
    "        assert peg_name in [\"SHOT_PEG\", \"REC_PEG\"], \"unexpected peg type\"\n",
    "        # convert to np.int32 from np.uint32 because pytorch cannot handle unsigned integers:\n",
    "        # since we'll feed this into pytorch eventually, it's better to snip this problem in the bud\n",
    "        pegs = np.array(h5data[peg_name]).flatten().astype(np.int32)\n",
    "        if decode_peg_id:\n",
    "            if peg_name == \"SHOT_PEG\":\n",
    "                if np.unique(h5data[\"SHOT_PEG\"]).size == 1:\n",
    "                    # pegs are sometimes broken (e.g. in the sudbury site), swap to SHOTID in that case\n",
    "                    pegs = np.array(h5data[\"SHOTID\"]).flatten().astype(np.int32)\n",
    "                else:\n",
    "                    assert np.unique(h5data[\"SHOT_PEG\"]).size == np.unique(h5data[\"SHOTID\"]).size\n",
    "                ids = pegs  # for shots, we can keep the full peg as the identifier (same result)\n",
    "            else:  # peg_name == \"REC_PEG\"\n",
    "                # for receiver pegs, the ID we want to keep is only the first few (base-10) digits\n",
    "                ids = pegs // (10 ** id_digit_count)\n",
    "        else:\n",
    "            ids = pegs\n",
    "        reversed_map = {sid: np.where(ids == sid)[0].astype(np.int32) for sid in np.unique(ids)}\n",
    "        assert sum([len(idxs) for idxs in reversed_map.values()]) == len(pegs), \"bad reverse map impl\"\n",
    "        return ids, reversed_map\n",
    "\n",
    "    def _get_gather_maps(\n",
    "        self,\n",
    "    ) -> typing.Tuple[typing.Sequence, typing.Dict[int, typing.Sequence[int]]]:\n",
    "        \"\"\"Returns both the trace-to-gather and gather-to-trace mappings for the dataset.\"\"\"\n",
    "        gather_to_trace_map = {}\n",
    "        trace_to_gather_map = [None] * self.total_trace_count\n",
    "        # first, double-loop on shot+traces to find intersections\n",
    "        for shot_id, shot_traces in self.shot_to_trace_map.items():\n",
    "            for line_id, line_traces in self.line_to_trace_map.items():\n",
    "                gather_traces = np.intersect1d(shot_traces, line_traces)\n",
    "                if not len(gather_traces):\n",
    "                    continue\n",
    "                next_gather_id = len(gather_to_trace_map)\n",
    "                gather_to_trace_map[next_gather_id] = gather_traces\n",
    "        # now, reloop over all gathers to assign ids in the direct map\n",
    "        for gather_id, gather_traces in gather_to_trace_map.items():\n",
    "            for trace_id in gather_traces:\n",
    "                assert 0 <= trace_id < self.total_trace_count, \"bad trace id found in gather?\"\n",
    "                assert trace_to_gather_map[trace_id] is None, \"trace had multiple intersections?\"\n",
    "                trace_to_gather_map[trace_id] = gather_id\n",
    "        assert not any([t is None for t in trace_to_gather_map]), \"there are useless traces?\"\n",
    "        return trace_to_gather_map, gather_to_trace_map\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_const_parameter(\n",
    "        h5data: h5py.Group,\n",
    "        field_name: typing.AnyStr\n",
    "    ) -> np.number:\n",
    "        \"\"\"Returns a constant-across-all-traces parameter from the HDF5 group.\"\"\"\n",
    "        data = np.unique(h5data[field_name])\n",
    "        assert len(data) == 1, f\"invalid field {field_name} (content not unique)\"\n",
    "        return data[0]\n",
    "\n",
    "    def _get_first_breaks(\n",
    "        self,\n",
    "        h5data: h5py.Group,\n",
    "        field_name: typing.AnyStr,\n",
    "    ) -> typing.Tuple[typing.Sequence[int], typing.Sequence[float]]:\n",
    "        \"\"\"Returns the maps of label indices and timestamps for first break picks.\"\"\"\n",
    "        assert self.samp_num > 0 and self.samp_rate > 0, \"invalid sample rate/count values\"\n",
    "        assert self.samp_num == h5data[\"data_array\"].shape[-1], \"bad trace data sample count\"\n",
    "        assert self.samp_rate >= 1000, \"current impl assumes rate is provided in usec, still true?\"\n",
    "        # we'll assume that fddata contains traces in msec, so we have to use the rate to convert\n",
    "        fbdata_msec = np.array(h5data[field_name]).flatten()\n",
    "        assert len(fbdata_msec) == self.total_trace_count, \"unexpected first break data array shape\"\n",
    "        samp_rate_msec = self.samp_rate / 1000.0\n",
    "        bad_fb_picks = np.where(fbdata_msec <= self.BAD_FIRST_BREAK_PICK_INDEX)\n",
    "        fbdata_idxs = self._get_first_break_indices(fbdata_msec, samp_rate_msec)\n",
    "        fbdata_idxs[bad_fb_picks] = -1\n",
    "        assert (fbdata_idxs < self.samp_num).all(), \"are picks really provided in msec?\"\n",
    "        return fbdata_idxs, fbdata_msec\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_first_break_indices(\n",
    "        fbp_times_in_milliseconds: np.ndarray,\n",
    "        sample_rate_in_milliseconds: float,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Returns the first break pick sample indices based on the annotations given in msec.\"\"\"\n",
    "        ratio = fbp_times_in_milliseconds / sample_rate_in_milliseconds\n",
    "        small_ratio_mask = np.bitwise_and(ratio > 0., ratio <= 1.)\n",
    "        fbp_indices = np.floor(ratio)\n",
    "        # make sure that small first break picks are not assigned to zero, which are flagged as abnormal.\n",
    "        fbp_indices[small_ratio_mask] = 1\n",
    "        return fbp_indices.astype(np.int32)\n",
    "\n",
    "    def _get_coords(\n",
    "        self,\n",
    "        h5data: h5py.Group,\n",
    "        target_set: typing.AnyStr\n",
    "    ) -> typing.Dict[int, typing.Sequence[float]]:\n",
    "        \"\"\"Returns the map of coordinates for either receivers or shots (based on `target_set`).\"\"\"\n",
    "        assert target_set in [\"REC\", \"SHOT\"], \"unexpected coords target set\"\n",
    "        if target_set == \"REC\":\n",
    "            target_fields = [\"REC_X\", \"REC_Y\", \"REC_HT\"]\n",
    "            target_map = self.rec_to_trace_map\n",
    "        else:\n",
    "            target_fields = [\"SOURCE_X\", \"SOURCE_Y\", \"SOURCE_HT\"]\n",
    "            target_map = self.shot_to_trace_map\n",
    "        coords = np.stack([np.array(h5data[field]).flatten() for field in target_fields], axis=1)\n",
    "        xy_scale, z_scale = (np.abs(float(self.coord_scale)), np.abs(float(self.ht_scale)))\n",
    "        coords = (coords / np.asarray((xy_scale, xy_scale, z_scale))).astype(np.float32)\n",
    "        out_coords_map = {}\n",
    "        for target_id, trace_ids in target_map.items():\n",
    "            unique_coords = np.unique(coords[trace_ids], axis=0)\n",
    "            if len(unique_coords) > 1:\n",
    "                # yes, it can sometimes happen, we just need to make sure it's only due to fp error\n",
    "                assert [\n",
    "                    np.allclose(unique_coords[0], unique_coords[i])\n",
    "                    for i in range(1, len(unique_coords))\n",
    "                ], \"unexpected coordinate variation inside group\"\n",
    "            out_coords_map[target_id] = unique_coords[0]\n",
    "        return out_coords_map\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of traces in the dataset.\"\"\"\n",
    "        return self.total_trace_count\n",
    "\n",
    "    def __getitem__(self, trace_id):\n",
    "        \"\"\"Returns a dictionary containing all pertinent information for a particular trace.\"\"\"\n",
    "        assert 0 <= trace_id < self.total_trace_count, \"trace query index is out-of-bounds\"\n",
    "        # for efficient gather-level parsing, a derived class should bypass this call entirely...\n",
    "        rec_id = self.trace_to_rec_map[trace_id]\n",
    "        shot_id = self.trace_to_shot_map[trace_id]\n",
    "        if self._h5fd is None:\n",
    "            self._h5fd = h5py.File(self.hdf5_path, mode=\"r\")\n",
    "        samples = self._h5fd[\"TRACE_DATA\"][\"DEFAULT\"][\"data_array\"][trace_id]\n",
    "        assert len(samples) == self.samp_num\n",
    "        return dict(  # dictionaries are used here to make sure we can batch results together\n",
    "            trace_id=trace_id,\n",
    "            shot_id=shot_id,\n",
    "            rec_line_id=self.trace_to_line_map[trace_id],\n",
    "            rec_id=rec_id,\n",
    "            gather_id=self.trace_to_gather_map[trace_id],\n",
    "            first_break_label=self.first_break_labels[trace_id],\n",
    "            first_break_timestamp=self.first_break_timestamps[trace_id],\n",
    "            rec_coords=self.rec_to_coords_map[rec_id],\n",
    "            shot_coords=self.shot_to_coords_map[shot_id],\n",
    "            samples=samples,\n",
    "            sample_count=int(self.samp_num),\n",
    "        )\n",
    "\n",
    "print(\"Trace dataset parser class declared and ready to be used!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We will now instantiate that class once for each dataset that we would like to parse. This may\n",
    "take a few minutes since each HDF5 file will be opened and scanned for metadata, but the loading\n",
    "of actual traces will happen later, meaning creating parsers like this will not demand a huge\n",
    "amount of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_parsers = {}\n",
    "\n",
    "for site_info in SITE_INFO_ARRAY:\n",
    "    data_file_path = os.path.join(data_root_path, site_info.hdf5_file_name)\n",
    "    parser = TraceDataset(\n",
    "        hdf5_path=data_file_path,\n",
    "        receiver_id_digit_count=site_info.receiver_id_digit_count,\n",
    "        first_break_field_name=site_info.first_break_field_name,\n",
    "    )\n",
    "    dataset_parsers[site_info.site_name] = parser\n",
    "\n",
    "print(\"All dataset parsers ready to load the data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are now ready to be used! As a final step, we show below how to assemble and display\n",
    "a line gather image using the parser's maps to fetch all the relevant trace IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lalor_parser = dataset_parsers[\"Lalor\"]\n",
    "gather_id = np.random.choice(list(lalor_parser.gather_to_trace_map.keys()))\n",
    "print(f\"Lalor has {len(lalor_parser.gather_to_trace_map)} line gathers; here's #{gather_id}:\")\n",
    "trace_ids = lalor_parser.gather_to_trace_map[gather_id]\n",
    "trace_data = [lalor_parser[tid] for tid in trace_ids]\n",
    "# we will stack the seismic amplitudes and normalize them in a trace-wise fashion\n",
    "trace_array = np.asarray([data[\"samples\"] for data in trace_data])\n",
    "trace_mean, trace_std = trace_array.mean(axis=1), (trace_array.std(axis=1) + 0.0001)\n",
    "trace_array = (trace_array - trace_mean.reshape(-1, 1)) / (trace_std * 2.5).reshape(-1, 1)\n",
    "trace_array = np.clip(trace_array, 0, 1)  # clip to make sure there is good contrast everywhere!\n",
    "# display the array as an image directly, and draw the picks on top of it\n",
    "plt.imshow(trace_array.T, aspect=\"auto\")\n",
    "valid_pick_coords = [\n",
    "    (idx, d[\"first_break_label\"])\n",
    "    for idx, d in enumerate(trace_data)\n",
    "    if d[\"first_break_label\"] > TraceDataset.BAD_FIRST_BREAK_PICK_INDEX\n",
    "]\n",
    "if len(valid_pick_coords):\n",
    "    # if any annotated picks are available, they will be drawn in red\n",
    "    picks_trace_idxs, picks_sample_idxs = tuple(zip(*valid_pick_coords))\n",
    "    plt.scatter(x=picks_trace_idxs, y=picks_sample_idxs, c='r', s=3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
