{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# First Break Picking Model Training Demo\n",
    "\n",
    "This notebook shows how to locally train a model to predict first breaks on one of the already-known\n",
    "survey sites. The AMLRT at Mila does not use notebooks very often, and this is why the code is\n",
    "mostly designed to be executed via scripts in IDEs such as PyCharm. Nevertheless, this notebook\n",
    "should give a good overview of the main steps required to train a model from scratch using the API.\n",
    "\n",
    "This example is also kept fairly minimal: it does not support the exploration of hyperparameters\n",
    "across multiple training sessions, and it does not support resuming an interrupted session, as that\n",
    "would be impractical in a notebook. We will however still highlight some key concepts surrounding\n",
    "hyperparameter exploration that are fundamental in the definition of experiments.\n",
    "\n",
    "Since we are training a model from scratch here for a significant number of epochs, it is strongly\n",
    "recommended to have a GPU on the machine where this notebook is executed. It will be detected and\n",
    "used by PyTorch below.\n",
    "\n",
    "Finally, note that this example will not use an external configuration file. Instead, it will\n",
    "directly invoke the functions and class constructors that would use the content of such a\n",
    "configuration file. This will help clarify the link between the content of these files, the role\n",
    "of each parameter, and the step where they are involved. An example of a valid configuration file\n",
    "(`unet-mini.yaml`) is also given in this folder, but it is meant to be executed in conjunction with\n",
    "the `hardpicks/main.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports are centralized here (helps identify environment issues before doing anything else!)\n",
    "\n",
    "# these packages are part of the 'standard' library, and should be available in all environments\n",
    "import functools\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# these packages are 3rd-party dependencies, and must be installed via pip or anaconda\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytorch_lightning.loggers\n",
    "import torch.utils.data\n",
    "\n",
    "# these packages are part of our API and must be manually installed (see the top-level README.md)\n",
    "import hardpicks\n",
    "import hardpicks.data.fbp.data_module as fbp_data_module\n",
    "import hardpicks.data.fbp.site_info as fbp_site_info\n",
    "import hardpicks.models.fbp.utils as model_utils\n",
    "import hardpicks.models.fbp.unet as fbp_unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's define the path to the folder where we'll be dumping all the training results/logs\n",
    "output_root_path = \"output/notebook_train_example/\"  # this path is relative to the notebook itself\n",
    "# make sure it doesn't exist already! (change the folder name otherwise)\n",
    "output_root_path = os.path.abspath(output_root_path)\n",
    "os.makedirs(output_root_path, exist_ok=False)\n",
    "print(f\"Experiment results will be in: {output_root_path}\")\n",
    "\n",
    "# next, we'll setup the high-level tensorboard stuff so that metrics/losses are logged somewhere\n",
    "tensorboard_output_path = os.path.join(output_root_path, \"tensorboard\")\n",
    "os.makedirs(tensorboard_output_path, exist_ok=True)\n",
    "tbx_logger = pytorch_lightning.loggers.TensorBoardLogger(\n",
    "    save_dir=tensorboard_output_path,\n",
    "    name=\"default\",\n",
    "    default_hp_metric=False,\n",
    ")\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "# the output folder and logging is now properly set up, we'll focus on the data next..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Loading\n",
    "\n",
    "The `main.py` script essentially prepares the output directory structure and loggers just like we\n",
    "did above, and then calls the `create_data_module` function of the\n",
    "`hardpicks/data/data_loader.py` module. Since we are trying to train a model\n",
    "using first break picking data, this function creates an `FBPDataModule` object that will parse a\n",
    "provided configuration dictionary and return a \"data loader factory\". The goal of using such a\n",
    "factory is to allow PyTorch Lightning to easily create new data loaders and use them across\n",
    "different processes while avoiding unnecessarily costly raw-data copies in memory. Here, since we\n",
    "will be training from scratch and invoking the creation of objects directly, we will avoid using\n",
    "the factory object and instead create the data loaders (and parsers) directly.\n",
    "\n",
    "A data loader in the context of PyTorch-based projects is essentially an object that loads and\n",
    "combines data samples into tensors so that a model can ingest them. This means that all the\n",
    "preprocessing and augmentation operations that must be applied to the raw data will be a\n",
    "responsibility of the data loader object.\n",
    "\n",
    "The data loader interface that is universally used for PyTorch-based project is\n",
    "[defined by PyTorch itself](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader).\n",
    "Most of the arguments it expects are related to how examples should be combined ('batched')\n",
    "together, how memory should be managed, or how many processes should be used in parallel. The key\n",
    "argument it expects is the `dataset` object it will be requesting examples from. That `dataset`\n",
    "object (which we call in practice a 'dataset parser', or simply a 'parser') is the object that\n",
    "holds all the logic required to read the raw data from the disk. It is also responsible for the\n",
    "cleaning and preprocessing of the raw data. In our case, the parser is based on the\n",
    "`ShotLineGatherDataset` class of the `hardpicks/data/fbp/gather_parser.py`\n",
    "module, and is further wrapped into cleaning and preprocessing classes. We show how to instantiate\n",
    "this parser below using existing API methods for a training and testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we need to identify which site we will be training on...\n",
    "# ... we will use the predefined site information arrays from the API to select Lalor for this demo\n",
    "lalor_site_info = fbp_site_info.get_site_info_by_name(\"Lalor\")\n",
    "assert os.path.isfile(lalor_site_info[\"raw_hdf5_path\"]), \\\n",
    "    f\"could not locate Lalor site raw data at: {lalor_site_info['raw_hdf5_path']}\"\n",
    "assert os.path.isfile(lalor_site_info[\"processed_hdf5_path\"]), \\\n",
    "    f\"could not locate Lalor site preprocessed data at: {lalor_site_info['processed_hdf5_path']}\"\n",
    "\n",
    "print(\"Training site info:\")\n",
    "for key, val in lalor_site_info.items():\n",
    "    print(f\"\\t{key}: {val}\")\n",
    "\n",
    "# we need to define right away what kind of segmentation class setup we want; we'll do binary!\n",
    "segmentation_class_count = 1  # one class of interest = first break, this is defined internally\n",
    "\n",
    "# next, we'll specify a few site-level hyperparameters that will influence how the data is processed\n",
    "# ...note that most of these hyperparameters can be omitted and they will default to proper values\n",
    "lalor_site_params = {\n",
    "    # this path points to a file that indicates bad gathers (by receiver/shot ids) to be avoided\n",
    "    \"rejected_gather_yaml_path\":\n",
    "        os.path.join(hardpicks.FBP_BAD_GATHERS_DIR, \"bad-gather-ids_combined.yaml\"),\n",
    "    # if we were to re-instantiate the parser often, we could cache its metadata, but no need here\n",
    "    \"use_cache\": False,\n",
    "    # all trace samples should be normalized in order to improve model training behavior\n",
    "    \"normalize_samples\": True,  # by default, the normalization will be independent for each trace\n",
    "    # do not use a 'buffer' zone around the annotated first break picks to make predictions easier\n",
    "    \"segm_first_break_buffer\": 0,\n",
    "}\n",
    "\n",
    "# the dataset parsers also need a couple hyperparameters to specify non-site-related settings\n",
    "generic_site_params = dict(\n",
    "    convert_to_fp16=True,  # convert trace sample data to 16-bit floats (saves memory!)\n",
    "    convert_to_int16=True,  # same as above, but for identifiers, picks, and other integer data\n",
    "    preload_trace_data=False,  # we cannot preload all the dataset into memory (it's too big!)\n",
    "    cache_trace_metadata=True,  # we can however cache its metadata into memory... (faster!)\n",
    "    provide_offset_dists=True,  # finally, we'll generate new offset distance arrays/maps\n",
    ")\n",
    "\n",
    "# during training, it's always good to use a couple of augmentation operations, so we'll define\n",
    "# them here (this is how the `ShotLineGatherPreprocessor` class expects to see them defined)\n",
    "lalor_site_augmentations = [\n",
    "    {\n",
    "        \"type\": \"crop\",\n",
    "        \"params\": {\n",
    "            \"low_sample_count\": 512,\n",
    "            \"high_sample_count\": 1024,\n",
    "            \"max_crop_fraction\": 0.333,\n",
    "        },\n",
    "    },\n",
    "    {\"type\": \"flip\"},\n",
    "]\n",
    "\n",
    "# all the data-related hyperparameters are ready, now let's create our training/validation parsers!\n",
    "\n",
    "lalor_eval_split_ratio = 0.15  # will split using 15% of shots for validation and 85% for training\n",
    "lalor_train_parser = fbp_data_module.FBPDataModule.create_parser(\n",
    "    site_info=lalor_site_info,\n",
    "    site_params={\n",
    "        **lalor_site_params,\n",
    "        \"augmentations\": lalor_site_augmentations,  # augmentations are only used for training!\n",
    "        \"subset\": {\"eval_ratio\": lalor_eval_split_ratio, \"use_eval_split\": False},\n",
    "    },\n",
    "    prefix=\"train\",\n",
    "    dataset_hyper_params=generic_site_params,\n",
    "    segm_class_count=segmentation_class_count,\n",
    ")\n",
    "print(f\"Training dataset parser ready with {len(lalor_train_parser)} gathers!\")\n",
    "\n",
    "lalor_valid_parser = fbp_data_module.FBPDataModule.create_parser(\n",
    "    site_info=lalor_site_info,\n",
    "    site_params={\n",
    "        **lalor_site_params,\n",
    "        \"subset\": {\"eval_ratio\": lalor_eval_split_ratio, \"use_eval_split\": True},\n",
    "    },\n",
    "    prefix=\"valid\",\n",
    "    dataset_hyper_params=generic_site_params,\n",
    "    segm_class_count=segmentation_class_count,\n",
    ")\n",
    "print(f\"Validation dataset parser ready with {len(lalor_valid_parser)} gathers!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# once we have our parsers, it's time to create the data loaders to get batches of gathers!\n",
    "# ...we need to use a special collate function to combine gathers of different sizes\n",
    "collate_fn = functools.partial(\n",
    "    fbp_data_module.fbp_batch_collate,\n",
    "    pad_to_nearest_pow2=True,\n",
    ")\n",
    "\n",
    "# with that collate function defined, we can create the two data loaders directly...\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=lalor_train_parser,\n",
    "    batch_size=6,\n",
    "    shuffle=True,  # always shuffle training data!\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "print(f\"Training data loader ready with {len(train_data_loader)} minibatches!\")\n",
    "\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=lalor_valid_parser,\n",
    "    batch_size=6,\n",
    "    shuffle=False,  # never shuffle validation/test data!\n",
    "    num_workers=2,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "print(f\"Validation data loader ready with {len(valid_data_loader)} minibatches!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have a data loader for both the training and validation splits of the Lalor data, we\n",
    "can display a couple examples for each, and prove that these two do not overlap based on the\n",
    "intersection of the loaded gather IDs.\n",
    "\n",
    "Note that each instance of a 'data sample' that can be loaded is a dictionary that contain\n",
    "a line gather (i.e. an array of traces recorded along a single continuous receiver line). This data\n",
    "comes with metadata, and everything is described using the following fields:\n",
    " - `Origin`: the name of the site that the data comes from (useful when we're mixing them!);\n",
    " - `shot_id`: the unique identifier of the shot that corresponds to the loaded gather;\n",
    " - `rec_line_id`: the unique identifier of the receiver line that corresponds to the loaded gather;\n",
    " - `rec_ids`: the unique identifier of the receivers that recorded the traces in the loaded gather;\n",
    " - `gather_id`: the unique identifier of the loaded gather (a 0-based int created by the parser);\n",
    " - `gather_trace_ids`: the unique identifier of the traces in the loaded sample;\n",
    " - `first_break_labels`: the array of first break pixel index labels (one for each trace);\n",
    " - `first_break_timestamps`: the array of first break timestamps (one for each trace);\n",
    " - `bad_first_breaks_mask`: a mask array that indicates which traces have an invalid first break label;\n",
    " - `rec_coords`: the array of receiver ground coordinates (one for each trace);\n",
    " - `rec_coords`: the array of shot ground coordinates (one for each trace);\n",
    " - `trace_count`: the number of traces in the loaded gather;\n",
    " - `sample_count`: the number of recorded seismic samples in each trace of the loaded gather;\n",
    " - `filled_first_breaks_mask`: a mask array that indicates which traces have interpolated labels;\n",
    " - `dead_rec_mask`: a mask array that indicates which traces have dead receivers (no amplitudes);\n",
    " - `samples`: the 2D array of recorded seismic samples (one vector per trace);\n",
    " - `segmentation_mask`: (if activated) the 2D segmentation class map used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's display an actual minibatch of random line gathers as a grid of 2D images\n",
    "fig, axes = plt.subplots(6, figsize=(9, 15))\n",
    "minibatch = next(iter(train_data_loader))\n",
    "for gather_idx in range(6):\n",
    "    # we'll use some utility functions that are already-written to convert gathers into images\n",
    "    gather_image = model_utils.generate_pred_image(\n",
    "        # note: the provided first breaks picks will be shown in green\n",
    "        batch=minibatch,\n",
    "        raw_preds=None,\n",
    "        batch_gather_idx=gather_idx,\n",
    "        segm_class_count=segmentation_class_count,\n",
    "        segm_first_break_prob_threshold=0.,\n",
    "        draw_prior=False,\n",
    "        draw_prob_heatmap=False,\n",
    "    )\n",
    "    ax = axes[gather_idx]\n",
    "    ax.set_xlabel(\"time (ms)\")\n",
    "    ax.set_xticks(np.linspace(0, gather_image.shape[1], num=9, dtype=np.int32))\n",
    "    real_sample_count = minibatch[\"samples\"][gather_idx].shape[1]\n",
    "    real_sample_rate = minibatch[\"sample_rate_ms\"][gather_idx]\n",
    "    ax.set_xticklabels(np.linspace(0, real_sample_count * real_sample_rate, num=9, dtype=np.int32))\n",
    "    ax.set_ylabel(\"receiver (index)\")\n",
    "    ax.set_yticks(np.linspace(0, gather_image.shape[0], num=5, dtype=np.int32))\n",
    "    real_trace_count = minibatch[\"samples\"][gather_idx].shape[0]\n",
    "    ax.set_yticklabels(np.linspace(0, real_trace_count, num=5, dtype=np.int32))\n",
    "    ax.imshow(\n",
    "        gather_image,\n",
    "        interpolation=\"none\",\n",
    "        aspect=\"auto\",\n",
    "    )\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# note that black borders in the displayed images are normal, these correspond to necessary padding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll do a quick intersection test to show that the metadata of the train/valid loaders is unique\n",
    "train_gather_ids = [\n",
    "    lalor_train_parser.get_meta_gather(idx)[\"gather_id\"]\n",
    "    for idx in range(len(lalor_train_parser))\n",
    "]\n",
    "valid_gather_ids = [\n",
    "    lalor_valid_parser.get_meta_gather(idx)[\"gather_id\"]\n",
    "    for idx in range(len(lalor_valid_parser))\n",
    "]\n",
    "assert len(np.intersect1d(train_gather_ids, valid_gather_ids)) == 0, \\\n",
    "    \"there is some overlap between the unique gather IDs of the two dataset parsers? oh-oh...\"\n",
    "print(\"All good, no intersection found!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Creation\n",
    "\n",
    "Once the data is loaded and ready to be used, we can create a model and prepare it for training. In\n",
    "theory, any kind of CNN with a semantic segmentation (encoder-decoder) setup should do the trick.\n",
    "We will use a fairly well-known and flexible U-Net architecture here.\n",
    "\n",
    "The important thing to note here is that since we're using PyTorch Lightning, the model has to be\n",
    "derived from PyTorch Lightning's [LightningModule class](https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html).\n",
    "This interface gives the model the responsibility of implementing what to do during both the\n",
    "training and validation steps. This means that it should manage its own loss function and define\n",
    "which optimizer and scheduler combo to use. Therefore, the hyperparameters for all these settings\n",
    "have to be provided to the model class's constructor, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    # hyperparameters that define the architecture of the model:\n",
    "    \"unet_encoder_type\": \"resnet18\",  # our CNN encoder will be based on the ResNet18 architecture\n",
    "    \"unet_decoder_type\": \"vanilla\",  # our CNN decoder will be based on the original U-Net architecture\n",
    "    \"encoder_block_count\": 5,  # the number of blocks that are defined by the encoder architecture\n",
    "    \"mid_block_channels\": 0,  # the number of (extra) blocks to use between the encoder and decoder\n",
    "    \"decoder_block_channels\": \"[256, 128, 64, 32, 16]\",  # the depth of the 1st conv layer in each decoder block\n",
    "    \"decoder_attention_type\": None,  # the type of attention layer to use in the decoder (experimental!)\n",
    "    \"segm_class_count\": segmentation_class_count,  # the segmentation class setup to use (1, 2, or 3)\n",
    "\n",
    "    # hyperparameters that define what data to use as input from the loaded minibatches:\n",
    "    \"use_dist_offsets\": True,  # toggles whether to use offset distances as extra input channels\n",
    "    \"use_first_break_prior\": False,  # toggles whether to use fbp priors as an extra input channel (experimental!)\n",
    "    \"coordconv\": False,  # toggles whether to use tensor space coordinates as extra input channels (experimental!)\n",
    "\n",
    "    # hyperparameters that define the optimizer/scheduler/loss setups:\n",
    "    \"optimizer_type\": \"Adam\",  # will use the Adam optimizer defined by PyTorch\n",
    "    \"optimizer_params\": {  # specifies the arguments to be passed to the optimizer's constructor\n",
    "        \"lr\": 0.002136,\n",
    "        \"weight_decay\": 0.000001,\n",
    "    },\n",
    "    \"scheduler_type\": \"StepLR\",  # will use the StepLR scheduler defined by PyTorch\n",
    "    \"scheduler_params\": {  # specifies the arguments to be passed to the scheduler's constructor\n",
    "        \"step_size\": 10,\n",
    "        \"gamma\": 0.1,\n",
    "    },\n",
    "    \"update_scheduler_at_epochs\": True,  # specifies that the scheduler should be updated each epoch\n",
    "    \"loss_type\": \"crossentropy\",  # will use the BinaryCrossEntropy loss defined by PyTorch\n",
    "    \"loss_params\": {},  # specifies the arguments to be passed to the loss function's constructor\n",
    "\n",
    "    # hyperparameters that define the evaluator configuration (for metrics):\n",
    "    \"use_full_metrics_during_training\": False,  # toggles whether to skip metric evaluation during training\n",
    "    \"eval_type\": \"FBPEvaluator\",  # type of the evaluator that will be instantiated\n",
    "    \"segm_first_break_prob_threshold\": 0.,  # minimum sensitivity threshold for first break predictions\n",
    "    \"eval_metrics\": [  # list of metrics that should be evaluated during validation (we'll use only two)\n",
    "        {\"metric_type\": \"HitRate\", \"metric_params\": {\"buffer_size_px\": 1}},\n",
    "        {\"metric_type\": \"MeanBiasError\"},\n",
    "    ],\n",
    "\n",
    "    # other hyperparameters:\n",
    "    \"gathers_to_display\": 10,  # specifies the number of gathers to render for tensorboard each epoch\n",
    "    \"use_checkpointing\": False,  # specifies whether to use gradient checkpointing to lower GPU memory usage\n",
    "    \"max_epochs\": 5,  # maximum number of epochs that training should run for\n",
    "}\n",
    "\n",
    "# let's instantiate the model using that config now!\n",
    "model = fbp_unet.FBPUNet(model_config)\n",
    "\n",
    "# last step: we'll give a reference to the tensorboard logger we created earlier to the model...\n",
    "setattr(model, \"_tbx_logger\", tbx_logger)\n",
    "\n",
    "param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Instantiated a U-Net model from scratch with {(param_count / 1000000):2.1f}M parameters.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "For the training itself, we also delegate the loop logic to PyTorch Lightning. The [trainer\n",
    "object](https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html) receives the data\n",
    "loaders and model, and it takes care of implementing the iterative forward/backward steps required\n",
    "to update model parameters and evaluate its performance. Below, we show the minimal code required\n",
    "to start this process; in the API, this is done in the `hardpicks/train.py`\n",
    "module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if a GPU is available in the current environment, we'll use it\n",
    "if torch.cuda.device_count():\n",
    "    print(\"Will train on GPU.\")\n",
    "else:\n",
    "    print(\"Will train on CPU.\")\n",
    "\n",
    "# if tensorboard is installed in your environment, you can launch it with the printed command!\n",
    "print(f\"Tensorboard can now be launched with:\\n\\t tensorboard --logdir {tensorboard_output_path}\")\n",
    "\n",
    "# this will save the 'best' model based on the hit rate evaluated by the model already\n",
    "checkpoint_callback = pytorch_lightning.callbacks.ModelCheckpoint(\n",
    "    dirpath=output_root_path,\n",
    "    filename=\"best-{epoch:03d}-{step:06d}\",\n",
    "    monitor=\"valid/HitRate1px\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "# finally, we can create the trainer object...\n",
    "trainer = pytorch_lightning.Trainer(\n",
    "    logger=tbx_logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    gpus=int(bool(torch.cuda.device_count())),\n",
    "    max_epochs=15,\n",
    ")\n",
    "# ... and start training!\n",
    "trainer.fit(\n",
    "    model=model,\n",
    "    train_dataloader=train_data_loader,\n",
    "    val_dataloaders=valid_data_loader,\n",
    ")\n",
    "\n",
    "# if we get here, training is done! (the 'fit' call above will block)\n",
    "best_model_path = os.path.abspath(checkpoint_callback.best_model_path)\n",
    "print(f\"Best model is saved at: {best_model_path}\")\n",
    "\n",
    "# to restore the best model and use it to generate predictions, we would use:\n",
    "# model = model.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "...now that you've got a trained model on a particular site, the next step would be to test it\n",
    "on another site! See the `fbp_predict_with_api.ipynb` notebook in the same folder for an example.\n",
    "\n",
    "There are two things we need to highlight about the kind of experiment we just ran here:\n",
    " - **We trained a model with a set of arbitrary hyperparameters that are likely suboptimal.** Doing\n",
    "   a proper hyperparameter search (using e.g. Orion) is highly recommended when trying to obtain\n",
    "   the maximum performance on a particular dataset. Using an off-the-shelf recipe (like we did here)\n",
    "   can offer a good starting point, but every experiment deserves its own fine-tuning.\n",
    " - **We trained and validated on the same survey site, and did not test the final 'best' model.**\n",
    "   There is no guarantee that the performance we see reported in tensorboard is not inflated due to\n",
    "   overfitting on some patterns found across the shots and receiver lines of Lalor. This is why it\n",
    "   is best to train on one site, validate on another, and finally test the 'best' models on another."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
